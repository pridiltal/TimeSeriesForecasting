---
title: "IM532 3.0 Applied Time Series Forecasting"
subtitle: "MSc in Industrial Mathematics"
author: "Dr. Thiyanga Talagala"
date: "8 March 2020"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: 
      - default
      - default-fonts
      - duke-blue
      - hygge-duke
      - libs/cc-fonts.css
      - libs/figure-captions.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

## Stochastic processes

"A statistical phenomenon that evolves in time according to probabilistic laws is called a stochastic process." (Box, George EP, et al. Time series analysis: forecasting and control.)

--
 
##  Non-deterministic time series or statistical time series

A sample realization from an infinite population of time series that could have been generated by a stochastic process. 

--

## Probabilistic time series model 

Let $\{X_1, X_2, ...\}$ be a sequence of random variables. Then the joint distribution of the random vector $[X_1, X_2, ..., X_n]'$ is 

$$P[X_1 \le x_1, X_2 \le x_2, ..., X_n \le x_n]$$
 

where $-\infty < x_1, ..., x_n < \infty$ and $n = 1, 2, ...$

---


## Mean function

--
The **mean function** of $\{X_t\}$ is

$$\mu_X(t)=E(X_t).$$
--
## Covariance function

The **covariance function** of $\{X_t\}$ is

$$\gamma_X(r, s)=Cov(X_r, X_s)=E[(X_r-\mu_X(r))(X_s-\mu_X(s))]$$

for all integers $r$ and $s$.

--

The covariance function of $\{X_t\}$ at lag $h$ is defined by
$$\gamma_X(h):=\gamma_X(h, 0)=\gamma(t+h, t)=Cov(X_{t+h}, X_t).$$



---

## Autocovariance function

The autocovariance function of ${X_t}$ at lag $h$ is

$$\gamma_X(h)=Cov(X_{t+h}, X_t).$$

--

## Autocorrelation function

The autocorrelation function of ${X_t}$ at lag $h$ is

$$\rho_X(h)=\frac{\gamma_X(h)}{\gamma_X(0)}=Cor(X_{t+h}, X_t).$$

---

## Weekly stationary

A time series $\{X_t\}$ is called weekly stationary if 

- $\mu_X(t)$ is independent of $t$.

- $\gamma_X(t+h, t)$ is independent of $t$ for each $h$.

In otherwords the statistical properties of the time series (mean, variance, autocorrelation, etc.) do not depend on the time at which the series is observed, that is no trend or seasonality. However,  a time series with cyclic behaviour (but with no trend or seasonality) is stationary.

--

## Strict stationarity of a time series

A time series $\{X_t\}$ is called weekly stationary if the random vector $[X_1, X_2..., X_n]'$ and $[X_{1+h}, X_{2+h}..., X_{n+h}]'$ have the same joint distribution for all integers $h$ and $n>0$.

---

# Simple time series models

## 1. iid noise

1. no trend or seasonal component

2. observations are independent and identically distributed (iid) random variables with zero mean. 

3. Notation: $\{X_t\} \sim IID(0, \sigma^2)$

4. plays an important role as a building block for more complicated time series.

---------------------------------------------------------------------------------
**Joint distribution function of iid noise**

$$P[X_1 \le x_1, X_2 \le x_2, ..., X_n \le x_n] = P[X_1 \le x_1]....P[X_n \le x_n].$$
--
Let $F_X(.)$ be the cumulative distribution function of each of the identically distributed random variables $X_1, X_2, ...$ Then, 
$$P[X_1 \le x_1, X_2 \le x_2, ..., X_n \le x_n] = F_X(x_1)...F_X(x_n)$$
--
There is no dependence between observations. Hence,
$$P[X_{n+h} \le x|X_1=x_1, ..., X_n=x_n] = P[X_{n+h} \le x].$$

---

## iid noise (cont.)

**Examples of iid noise**

1. Sequence of iid random variables $\{X_t, t=1, 2,...\}$ with $P(X_t=1)=\frac{1}{2}$ and $P(X_t = 0)=\frac{1}{2}.$

2. 

```{r, comment=NA}
rnorm(15)
```

--

## Question

Let $\{X_t\}$ is a iid noise with $E(X^2)=\sigma^2<\infty$. Show that $\{X_t\}$ is a stationary process.

---
# Simple time series models

## 2. White noise

If $\{X_t\}$ is a sequence of uncorrelated random variables, each with zero mean and variance $\sigma^2$, then such a sequence is referred to as **white noise**.

Note: Every $IID(0, \sigma^2)$ sequence is $WN(0, \sigma^2)$ but not conversely.

---
# Simple time series models


## 3. Random walk

A random walk process is obtained by cumulatively summing iid random variables. If $\{S_t, t=0, 1, 2, ...\}$ is a random walk process, then
$S_0 =0$

$S_1=0+X_1$

$S_2=0+X_1+X_2$

$...$

$S_t=X_1+X_2+...+X_t.$

--

## Question

Is $\{S_t, t=0, 1, 2, ...\}$ a weak stationary process?

---

## Identifying non-stationarity in the mean

- Using time series plot

- ACF plot

   - ACF of stationary time series will drop to relatively quickly.
   
   - The ACF of non-stationary series decreases slowly.
   
   - For non-stationary series, the ACF at lag 1 is often large and positive.


---

# Elimination of Trend and Seasonality by Differencing

- Differencing helps to stabilize the mean.

## Backshift notation: 

$$BX_t=X_{t-1}$$

## Ordinary differencing

The first-order differencing can be defined as

$$\nabla X_t = X_t-X_{t-1}=X_t-BX_t=(1-B)X_t$$
The second-order differencing

$$\nabla^2X_t=\nabla(\nabla X_t)=\nabla(X_t-X_{t-1})=\nabla X_t - \nabla X_{t-1}=X_t-2X_{t-1}+X_{t-2}$$
- In practice, we seldom need to go beyond second order differencing. 


---
## Seasonal differencing

- differencing between an observation and the corresponding observation from the previous year.

$$\nabla_mX_t=X_t-X_{t-m}=(1-B^m)X_t$$
where $m$ is the number of seasons. For monthly, $m=12$, for quarterly $m=4$.

For monthly series

$$\nabla_{12}X_t=X_t-X_{t-12}$$

Twice-differenced series

$$\nabla^2_{12}X_t=\nabla_{12}X_t-\nabla_{12}X_{t-1}=(X_t-X_{t-12})-(X_{t-1}-X_{t-13})$$

---

All rights reserved by [Thiyanga S. Talagala](https://thiyanga.netlify.com/)
