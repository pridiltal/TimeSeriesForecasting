---
title: "IM532 3.0 Applied Time Series Forecasting"
subtitle: "MSc in Industrial Mathematics"
author: "Thiyanga Talagala"
date: "5 May 2020"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: 
      - default
      - default-fonts
      - duke-blue
      - hygge-duke
      - libs/cc-fonts.css
      - libs/figure-captions.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

# Necessary R packages

```{r, comment=NA}
library(forecast)
library(fpp2)
```
---
# Recap: Stationarity

.pull-left[

```{r, message=FALSE}
library(forecast)
set.seed(20205)
ts.wn <- as.ts(rnorm(20))
autoplot(ts.wn)
```
]

.pull-right[

```{r}
ggAcf(ts.wn)
```

]

White noise implies stationarity.

---

# Recap: Stationarity

.pull-left[

```{r, message=FALSE}
library(fpp2)
autoplot(uschange[,"Consumption"], main="Changes in US consumption and income")
```
]

.pull-right[

```{r}
ggAcf(uschange[,"Consumption"])
```

]

Stationarity does not imply white noise.
---
# Non-Stationary Time Series

**1. Deterministic trend**

$$Y_t  = f(t) + \epsilon_t$$


where $\epsilon_t \sim iid(0, \sigma^2)$, $t = 1, 2, ...T$

Mean of the process is time dependent, but the variance of the process is constant.

**2. Random walk** 

$$Y_t = Y_{t-1} + \epsilon_t$$

- Random walk has a stochastic trend.

- Model behind naive method.

**3. Random walk with drift**

$$Y_t = \alpha+  Y_{t-1} + \epsilon_t$$

- Random walk with drift has a stochastic trend and a deterministic trend.

- Model behind drift method.
---

# Random walk


$$
\begin{aligned}
  Y_t &= Y_{t-1} + \epsilon_t \\
     Y_1    &= Y_0 + \epsilon_1 \\
         Y_2 &=  Y_1 + \epsilon_1=Y_0 + \epsilon_1 + \epsilon_2\\
          Y_3 &=  Y_2 + \epsilon_3=Y_0 + \epsilon_1 + \epsilon_2 +\epsilon_3\\
          .   \\
          Y_t &=Y_{t-1} + \epsilon_t=Y_0 + \epsilon_1 + \epsilon_2 + \epsilon_3 +...+ \epsilon_t = Y_0 + \sum_{i=1}^{t} \epsilon_t
\end{aligned}
$$

Mean: $E(Y_t) = Y_0$.

Variance: $Var(Y_t)=t \sigma^2$.



---

## Random walk generation 

.pull-left[
```{r, comment=NA}
# method 1
set.seed(100)
n <- 100
error <- rnorm(n, sd=2)
# assume y0=0 (starting at 0)
rw.ts <- cumsum(error)
```

```{r, comment=NA, fig.height=4}
autoplot(as.ts(rw.ts))
```

]



.pull-right[
```{r, comment=NA}

# method 2
rw.ts <- rep(0, n)
for (i in seq.int(2, n)){
  rw.ts[i] <- rw.ts[i-1] + error[i]
}
rw.ts <- rw.ts 

```

```{r, comment=NA, fig.height=4}
autoplot(as.ts(rw.ts))
```
]

---

# Random walk with drift


$$
\begin{aligned}
  Y_t &= Y_{t-1} + \epsilon_t \\
     Y_1    &= \alpha+Y_0 + \epsilon_1 \\
         Y_2 &= \alpha+ Y_1 + \epsilon_1=2 \alpha+Y_0 + \epsilon_1 + \epsilon_2\\
          Y_3 &= \alpha+ Y_2 + \epsilon_3= 3 \alpha+ Y_0 + \epsilon_1 + \epsilon_2 +\epsilon_3\\
          .   \\
          Y_t &= \alpha+Y_{t-1} + \epsilon_t= t \alpha+ Y_0 + \epsilon_1 + \epsilon_2 + \epsilon_3 +...+ \epsilon_t = t \alpha + Y_0 + \sum_{i=1}^{t} \epsilon_t
\end{aligned}
$$

It has a *deterministic trend* $(Y_0 + t \alpha)$ and a *stochastic trend* $\sum_{i=1}^{t} \epsilon_t$.

Mean: $E(Y_t) = Y_0 + t\alpha$

Variance: $Var(Y_t) = t\sigma^2$.

There is a trend in both mean and variance. 


---

# Simulate a random walk with drift

```{r, fig.height=5}
n <- 200
error <- rnorm(n, sd=2)
alpha <- 2
y1 <- rep(0, n)
for (i in seq.int(2, n)){
  y1[i] <- alpha + y1[i-1] + error[i]
}

autoplot(as.ts(y1))
```

---
## Common trend removal (de-trending) procedures

1. Deterministic trend: Time-trend regression

      The trend can be removed by fitting a deterministic polynomial time trend. The residual series after removing the trend will give us the de-trended series.

1. Stochastic trend: Differencing
 
      The process is also known as a **Difference-stationary process**.

      
# Notation: I(d)

Integrated to order $d$: Series can be made stationary by differencing $d$ times.
 
 - Known as $I(d)$ process.
 

**Question: ** Show that random walk process is an $I(1)$ process.

The random walk process is called a unit root process.
(If one of the roots turns out to be one, then the process is called unit root process.)

---

# Variance stabilization

Eg:

- Square root: $W_t = \sqrt{Y_t}$

- Logarithm: $W_t = log({Y_t})$

     - This very useful.
     
     - Interpretable: Changes in a log value are **relative (percent) changes on the original sclae**.
     


---

### Monthly Airline Passenger Numbers 1949-1960

.pull-left[

**Without transformations**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(forecast)
library(fpp2)
data(elec)
autoplot(AirPassengers)+ylab("Monthly Airline Passenger Numbers 1949-1960")
```

]

.pull-right[

**Square root transformation**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(forecast)
library(fpp2)
data(elec)
autoplot(sqrt(AirPassengers))+ylab("sqrt(Monthly Airline Passenger Numbers 1949-1960)")
```
]
---


### Australian monthly electricity production: Jan 1956 – Aug 1995

.pull-left[

**Without transformations**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(forecast)
library(fpp2)
data(AirPassengers)
autoplot(AirPassengers)+ylab("Monthly Airline Passenger Numbers 1949-1960")
```

]

.pull-right[

**Logarithm transformation**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(forecast)
library(fpp2)
data(AirPassengers)
autoplot(log(AirPassengers))+ylab("log(Monthly Airline Passenger Numbers 1949-1960)")
```
]

---

# Box-Cox transformation

$$
  w_t=\begin{cases}
    log(y_t), & \text{if $\lambda=0$} \newline
    (Y_t^\lambda - 1)/ \lambda, & \text{otherwise}.
  \end{cases}
$$


Different values of $\lambda$ gives you different transformations.

- $\lambda=1$: No **substantive** transformation

- $\lambda = \frac{1}{2}$: Square root plus linear transformation

- $\lambda=0$: Natural logarithm

- $\lambda = -1$: Inverse plus 1

Balance the seasonal fluctuations and random variation across the series.

---

# Box-Cox transformation: R codes

**BoxCox.lambda: Automatic selection of Box Cox transformation parameter**

```{r, warning=FALSE, message=FALSE, comment=NA}
forecast::BoxCox.lambda(AirPassengers)
```

Some times this value is not sensible.

**BoxCox: Transformation of the input variable using a Box-Cox transformation**

```r
lambda <- forecast::BoxCox.lambda(AirPassengers)
w <- BoxCox(AirPassengers, lambda)
```

You can pass a user-defined value for `lambda`.

**InvBoxCox: Reverse transformation**

```r
InvBox(w)
```

---

## Box-Cox transformation on `AirPassengers`

.pull-left[

**Without transformations**

```r
autoplot(AirPassengers)+ylab("Monthly Airline Passenger Numbers 1949-1960")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(forecast)
library(fpp2)
data(AirPassengers)
autoplot(AirPassengers)+
  ylab("Monthly Airline Passenger Numbers 1949-1960")
```

]

.pull-right[

**Box-Cox transformation**

```r
lambda <- forecast::BoxCox.lambda(AirPassengers)
*autoplot(BoxCox(AirPassengers, lambda))+
ylab("Monthly Airline Passenger Numbers 1949-1960")
```



```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(forecast)
library(fpp2)
lambda <- forecast::BoxCox.lambda(AirPassengers)
autoplot(BoxCox(AirPassengers, lambda))+ylab("Monthly Airline Passenger Numbers 1949-1960")
```
]

What differences do you notice in the scale?
---

## Note: Box-Cox Transformation

- If $\lambda = 0$? 

    - Behaves like log transformation. 
    
    - Force forecasts to be positive.
--

- If $\lambda =1$? No transformation is needed.
--

- If some $Y_t = 0$?, then must have $\lambda > 0$.
--
- If some $Y_t < 0$? Use power transformation or adjust the time series **by adding a constant to all values.**

--

- Choose a simple value of $\lambda$. It makes explanations easier.

--

- Transformation oftem makes little difference to forecasts but has large effects on PI.

---
# Application

`snaive` + applying BoxCox transformation

.pull-left[

```{r, comment=NA, message=FALSE, warning=FALSE, fig.height=6}
fit <- snaive(AirPassengers, lambda = 0)
autoplot(fit)
```
]

.pull-right[

## Steps: 

✅ apply Box-Cox transformation.

✅ fit a model.

✅ reverse transformation.

]

<!-- R will do the Box-Cox transformation, Fit model, back transformation-->

---

## What differences do you notice?

.pull-left[

```{r, comment=NA, message=FALSE, warning=FALSE, fig.height=6, echo=FALSE}
autoplot(AirPassengers)
```
]

.pull-right[

```{r, comment=NA, message=FALSE, warning=FALSE, fig.height=6, echo=FALSE}
autoplot(cangas)
```


]

<!--Monotonically increasing variance vs Non-monotonically increasing variance. Any monotonic transformation wouldn't work here. When you apply boxcox transformation it will transform one part and do the opposite for the other part. There are different ways to handle this. What transformation would work for cangas data set. Video:sd1-4 (48) -->

---

## ARMA(p, q) model


$$Y_t=c+\phi_1Y_{t-1}+...+\phi_p Y_{t-p}+ \theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}+\epsilon_t$$

- These are stationary models.

- They are only suitable for **stationary series**.

## ARIMA(p, d, q) model

Differencing --> ARMA

**Step 1: Differencing**

$$Y'_t = (1-B)^dY_t$$

**Step 2: ARMA**

$$Y'_t=c+\phi_1Y'_{t-1}+...+\phi_p Y'_{t-p}+ \theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}+\epsilon_t$$

---

# Time series simulation

$$Y_t = 0.8Y_{t-1}+\epsilon_t$$

```{r, comment=NA, message=FALSE, warning=FALSE}
set.seed(2020)
ts.sim <- arima.sim(list(order = c(1,0,0), ar = 0.8), n = 50)
ts.sim
```

> Use `?arima.sim` to view more examples.

---

# Automated ARIMA algorithm

```{r, comment=NA, message=FALSE, warning=FALSE}
Arima(ts.sim, order=c(1, 0, 0), include.mean = FALSE)
```

<!--Not exactly equals to 0.8, due to an estimation error.-->
---



# Interpretation of R output: ARIMA model 

.content-box-yellow[Intercept form]

Using the backshift notation.

$$(1-\phi_1B-...-\phi_pB^p)Y'_t=c+(1+\theta_1B+...+\theta_qB^q)\epsilon_t$$

.content-box-yellow[Mean form]

$$(1-\phi_1B-...-\phi_pB^p)(Y'_t-\mu)=c+(1+\theta_1B+...+\theta_qB^q)\epsilon_t$$
Where,

 - $Y'_t=(1-B)^dY_t$
 
 - $\mu = E(Y'_t)$, when $d \neq 0$, otherwise $\mu = E(Y'_t)$.
 
 - $c = \mu(1-\phi_1 - ... - \phi_p)$
 
 R always return an estimate of $\mu$. 
 
 
 <!--How constant relate to the mean of the process. These forms are equivalent. When you can relate c to mu. ARMA models are fitted to the stationary data. Hence, we get mean of stationary data. Data can be stationary in two ways, on it's own or by taking the differencing. If differencing is applied, then you get mean of differenced series, otherwise you get mean of original series.-->


---

**1. Generate data from a model with a drift**

$$Y_t = 10+0.5Y_{t-1}+\epsilon_t$$
```{r, comment=NA}
set.seed(22)
y <- 10 + arima.sim(list(ar=.8),n=200)
```


**2. Fit an ARIMA model to generated data**

```{r, comment=NA}
Arima(as.ts(y),order=c(1,0,0))
```

$E(Y_t)=\frac{\phi_0}{1-\phi_1}$

$\phi_0 = 8.8345 \times (1-0.0384) = 9.575901$


---

```{r, comment=NA, warning=FALSE, message=FALSE}

ts.sim2 <- arima.sim(list(order = c(0,1,0)), n = 50)
```

```{r, comment=NA, message=FALSE, warning=FALSE}
Arima(ts.sim2, order=c(0, 1, 0))
```

---

# `Arima` in R

1. When $d=0$, provides estimate of $\mu = E(Y_t)$.

2. Default setting is `include.mean=TRUE`. Setting `includemean=FALSE` will force $c=\mu=0$.

3. When $d > 0$ sets $c=\mu=0$.

4. When $d=1$ setting `include.drift=TRUE`, estimates $\mu\neq0$ as `drift`.

5. When $d>1$ no constant is allowed.

---
class: duke-orange, center, middle

# Identifying suitable ARIMA models

---

# Modelling steps

1. Plot the data.

--
2. If necessary, transform the data (using a Box-Cox transformation) to stabilise the variance.
--

3. If the data are non-stationary, take first differences of the data until the data are stationary.
--

4. Examine the ACF/PACF to identify a suitable model.
--

5. Try your chosen model(s), and use the AICc to search for a better model.
--

6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.

--
7. Once the residuals look like white noise, calculate forecasts.

Source: Forecasting: Principles and Practice, Rob J Hyndman and George Athanasopoulos

---
# Step 0: Split time series into training and test sets

```{r, comment=NA}
training.ap <- window(AirPassengers, end=c(1957, 12))
training.ap
test.ap <- window(AirPassengers, start=c(1958, 1))
test.ap
```

---

```{r}
autoplot(AirPassengers) + 
  geom_vline(xintercept = 1958, colour="forestgreen")

```

---

# Step 1: Plot data

1. Detect unusual observations in the data

1. Detect non-stationarity by visual inspections of plots

Stationary series:

- has a constant mean value and fluctuates around the mean.

- constant variance.

- no pattern predictable in the long-term.

---

# Step 1: Plot data (cont.)

.pull-left[
```{r, comment=NA, warning=FALSE, message=FALSE}
autoplot(training.ap)
```
]

.pull-right[

1. Transformations help to stabilize the variance.

1. Differencing helps to stabilize the mean.


.content-box-yellow[

1. Need transformations?

2. Need differencing?

]

]

---

# Step 2: Apply transformations

```{r, comment=NA, warning=FALSE, fig.height=5, fig.width=5, message=FALSE}
log.airpassenger <- log(training.ap)
#log.airpassenger <- BoxCox(training.ap, lambda = 0)
autoplot(log.airpassenger)

```

---

## Step 3: Take difference series

### Identifying non-stationarity by looking at plots

- Time series plot

- The ACF of stationary data drops to zero relatively quickly.

- The ACF of non-stationary data decreases slowly.

- For non-stationary data, the value of $r_t$ is often large and positive.

--
### Recap: Non-seasonal differencing and seasonal differencing

** Non seasonal first-order differencing:** $Y'_t=Y_t - Y_{t-1}$

<!--Miss one observation-->

**Non seasonal second-order differencing:** $Y''_t=Y'_t - Y'_{t-1}$

<!--Miss two observations-->

**Seasonal differencing:** $Y_t - Y_{t-m}$

<!--To get rid from prominent seasonal components. -->

- For monthly, $m=12$, for quarterly, $m=4$.

<!--We will loosefirst 12 observations-->


- Seasonally differenced series will have $T-m$ observations.
<!--Usually we do not consider differencing more than twice. -->

> There are times differencing once is not enough. However, in practice,it is almost never necessary to go beyound second-order differencing.

<!--Even the second-order differencing is very rare.-->

---
# Step 3: Take difference series (cont.)

🙋

.content-box-yellow[Seasonal differencing or Non-seasonal differencing?]

<!--Take seasonal differencing first. Seasonal differencing might be enough, you do not need to do further differencing. First order seasonal differencing never removes the seasonal effect.-->

🙋

.content-box-yellow[Interpretation of differencing?]

<!--It is important that if the differencing is used, the differences are interpretable. -->

<!--First differences are the change between one observation and the next. Changes of index. Second order differencing is changes of changes.-->

<!--Seasonal differences are the change between one year to the next.-->

<!--But taking lag 3 differences for yearly data, for example, results in a model which cannot be sensible interpreted.-->

<!--It is important that the differencing are interpretable.-->
---

# Step 3: Take difference series (cont.)

.pull-left[

```{r, comment=NA, warning=FALSE, message=FALSE}

autoplot(log.airpassenger)

```

]

.pull-right[
```{r, comment=NA, warning=FALSE, message=FALSE}

ggAcf(log.airpassenger)

```

]

---
# Step 3: Take difference series (cont.)

## Operations of differencing

.pull-left[

```{r, comment=NA, warning=FALSE, message=FALSE}

autoplot(log.airpassenger)

```

]

.pull-right[
```{r,comment=NA, message=FALSE,  warning=FALSE}
library(magrittr) # to load %>%
#autoplot(diff(log.airpassenger,lag=12))
log.airpassenger %>% diff(lag=12)  %>% autoplot()

```

Does this look stationary?
]

<!--Strongseasonal component is now vanished. Does this look stationary? -->

---
# Step 3: Take difference series (cont.)

.pull-left[
```{r,comment=NA, message=FALSE,  warning=FALSE}
library(magrittr) # to load %>%
#autoplot(diff(log.airpassenger,lag=12))
log.airpassenger %>% diff(lag=12)  %>% autoplot()

```

]


.pull-right[
```{r,comment=NA, message=FALSE,  warning=FALSE}
log.airpassenger %>% 
  diff(lag=12) %>% 
  ggAcf()

```

]

---

# Step 3: Take difference series (cont.)

.pull-left[
```{r,comment=NA, message=FALSE,  warning=FALSE}
library(magrittr) # to load %>%
#autoplot(diff(log.airpassenger,lag=12))
log.airpassenger %>% 
  diff(lag=12) %>% 
  diff(lag=1)  %>% 
  autoplot()

```

<!--Code for second-order differencing: 
log.airpassenger %>% 
  diff(lag=12) %>% 
  diff(lag=1)  %>% 
  diff(lag=1) %>%
  autoplot() -->

]


.pull-right[
```{r,comment=NA, message=FALSE,  warning=FALSE}
log.airpassenger %>% 
  diff(lag=12) %>% 
  diff(lag=1)  %>% 
  ggAcf()

```

]
---

# Testing for nonstationarity for the presence of unit roots


- Dickey and Fuller (DF) test

- Augmented DF test

- Phillips and Perron (PP) nonparametric test

-  Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test

---

# KPSS test

**H0:** Series is level or trend stationary.

**H1:** Series is not stationary.


```{r, comment=NA, message=FALSE, warning=FALSE}
library(urca)
diff.sdiff.log.passenger <- log.airpassenger %>% 
  diff(lag=12) %>% 
  diff(lag=1)

diff.sdiff.log.passenger %>%
  ur.kpss() %>%
  summary()
```

---

# KPSS test 

```{r, comment=NA}
ur.kpss(log.airpassenger) %>% summary()
```

```{r, comment=NA}
sdiff.log.airpassenger <- training.ap %>% log() %>% diff(lag=12)
ur.kpss(sdiff.log.airpassenger) %>% summary()
```

<!--This gives an idea about only non-seasonal differencing-->

---

# How many differences you need to take?

Non-seasonal: Automatically selecting differences.

<!--With this command you do not need to run KPSS test. It will automatically run the test inside and returns you how many times you need to do differencing.-->

```{r, comment=NA}

ndiffs(log.airpassenger)

```

```{r, comment=NA}

ndiffs(sdiff.log.airpassenger)
```

```{r, comment=NA}
ndiffs(diff.sdiff.log.passenger)
```
---
# How many differences you need to take? (cont.)

Seaonal - Automatically selecting differences.

.pull-left[
STL decomposition: $Y_t = T_t + S_t + R_t$

Strength of seasonality: 

$$F_s = max \left(0, 1-\frac{Var(R_t)}{Var(S_t + R_t)}\right)$$

```{r, comment=NA}
nsdiffs(log.airpassenger)
```

```{r, comment=NA}
log.airpassenger %>% diff(lag=1) %>% nsdiffs()
```

```{r, comment=NA}
nsdiffs(sdiff.log.airpassenger)
```
]

.pull-right[
```{r, comment=NA}
training.ap %>% stl(s.window = 12) %>% autoplot()

```

]
<!--This is for number of seasonal differencing needed.-->

---
# Step 4: Examine the ACF/PACF to identify a suitable model

```{r, comment=NA, warning=FALSE, message=FALSE}
ggtsdisplay(diff.sdiff.log.passenger)
```

---
background-image: url('sar.PNG')
background-position: center
background-size: contain
---
## AR(p)

- ACF dies out in an exponential or damped
sine-wave manner.

- there is a signicant spike at lag $p$ in PACF, but
none beyond $p$.

## MA(q)

- ACF has all zero spikes beyond the $q^{th}$ spike.

- PACF dies out in an exponential or damped
sine-wave manner.

## Seasonal components

- The seasonal part of an AR or MA model will be seen
in the seasonal lags of the PACF and ACF.

.pull-left[
**ARIMA(0,0,0)(0,0,1)12 will show**
 
  - a spike at lag 12 in the ACF but no other significant spikes.

  - The PACF will show exponential decay in the seasonal lags  12, 24, 36, . . . .
]

.pull-right[
**ARIMA(0,0,0)(1,0,0)12 will show**

  - exponential decay in the seasonal lags of the ACF.
    
  - a single significant spike at lag 12 in the PACF.
]
---
background-image: url(Akaike.jpg)
background-size: 100px
background-position: 98% 6%

# Information criteria


- Akaike's Information Criterion (AIC)

$$AIC = -2log(L)+2(p+q+k+1)$$
where $L$ is the likelihood of the data, $k=1$ if $c\neq 0$ and $k=0$ if $c=0$.

- Corrected AIC

$$AICc=AIC + \frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}$$
-Bayesian Information Criterion

$$BIC=AIC+[log(T)-2](p+q+k-1)$$

 - Good models are obtained by minimizing either the $AIC, AICc$ or $BIC$. 
 
 - Our preference is to use the $AICc$.


---

## Step 4: Examine the ACF/PACF to identify a suitable model (cont.)

```{r, comment=NA,  warning=FALSE, message=FALSE}
ggtsdisplay(diff.sdiff.log.passenger)
```

---

## Step 4: Examine the ACF/PACF to identify a suitable model (cont.)

- $d=1$ and $D=1$ (from step 3)

- Significant spike at lag 1 in ACF suggests
non-seasonal MA(1) component.

- Significant spike at lag 12 in ACF suggests seasonal
MA(1) component.

- Initial candidate model: $ARIMA(0,1,1)(0,1,1)_{12}$.

- By analogous logic applied to the PACF, we could also have started with $ARIMA(1,1,0)(1,1,0)_{12}$.

- Let's try both
<!--Since the second is not significant, I did not consider 3. I started with the simplest.-->

---

**Initial model:**

$ARIMA(0,1,1)(0,1,1)_{12}$

$ARIMA(1,1,0)(1,1,0)_{12}$

**Try some variations of the initial model:**

$ARIMA(0,1,1)(1,1,1)_{12}$

$ARIMA(1,1,1)(1,1,0)_{12}$

$ARIMA(1,1,1)(1,1,1)_{12}$


Both the ACF and PACF show significant spikes at lag 3, and almost significant spikes at lag 3, indicating that some additional non-seasonal terms need to be included in the model.

$ARIMA(3,1,1)(1,1,1)_{12}$

$ARIMA(1,1,3)(1,1,1)_{12}$

$ARIMA(3,1,3)(1,1,1)_{12}$
---
.pull-left[
```{r, comment=NA}
fit1 <- Arima(training.ap, 
              order=c(0,1,1),
seasonal=c(0,1,1), lambda = 0)
fit1
```
]

---

```{r, comment=NA}
fit2 <- Arima(training.ap, 
              order=c(1,1,0),
seasonal=c(1,1,0), lambda = 0)
fit2
```

---
```{r, comment=NA}
fit3 <- Arima(training.ap, 
              order=c(0,1,1),
seasonal=c(1,1,1), lambda = 0)
fit3
```
---

```{r, comment=NA}
fit4 <- Arima(training.ap, 
              order=c(1,1,1),
seasonal=c(1,1,0), lambda = 0)
fit4
```
---

```{r, comment=NA}
fit5 <- Arima(training.ap, 
              order=c(1,1,1),
seasonal=c(1,1,1), lambda = 0)
fit5
```
---

```{r, comment=NA}
fit6 <- Arima(training.ap, 
              order=c(3,1,1),
seasonal=c(1,1,1), lambda = 0)
fit6
```

---

```{r, comment=NA}
fit7 <- Arima(training.ap, 
              order=c(1,1,3),
seasonal=c(1,1,1), lambda = 0)
fit7
```

---
```{r, comment=NA}
fit8 <- Arima(training.ap, 
              order=c(3,1,3),
seasonal=c(1,1,1), lambda = 0)
fit8
```

---
**Initial model: AICc**

$ARIMA(0,1,1)(0,1,1)_{12}$: -344.33 (the smallest AICc)

$ARIMA(1,1,0)(1,1,0)_{12}$: -336.32

**Try some variations of the initial model:**

$ARIMA(0,1,1)(1,1,1)_{12}$: -342.3 (second smallest AICc)

$ARIMA(1,1,1)(1,1,0)_{12}$: -336.08

$ARIMA(1,1,1)(1,1,1)_{12}$: -340.74

$ARIMA(3,1,1)(1,1,1)_{12}$: -338.89 

$ARIMA(1,1,3)(1,1,1)_{12}$: -339.42 

$ARIMA(3,1,3)(1,1,1)_{12}$: -335.65

---

# Step 6: Residual diagnostics

## Fitted values: 

$\hat{Y}_{t|t-1}$: Forecast of $Y_t$ based on observations $Y_1,...Y_t$.


## Residuals

$$e_t=Y_t - \hat{Y}_{t|t-1}$$

### Assumptions of residuals

- $\{e_t\}$ uncorrelated. If they aren't, then information left in residuals that should be used in computing forecasts.

<!--If you see autocorrelations, then you should go back and adjust residuals. In theoretically, If there is information leftover and we can do something better. But it is not the case you will also be able to do with. If can't you can't. Then stop. If you check you know you have done the best as you can.-->

- $\{e_t\}$ have mean zero. If they don't, then forecasts are biased.

<!--If you see autocorrelations, then you should go back and adjust residuals. We want our residuals to be unbiased. If the mean is not zero. Go and adjust the model. Add an intercept. Whatever you want to do.-->

### Useful properties (for prediction intervals)

- $\{e_t\}$ have constant variance.

- $\{e_t\}$ are normally distributed.

<!--If the following assumptions are wrong that doesn't mean your forecasts are incorrect. -->


---
# Step 6: Residual diagnostics (cont.)

H0: Data are not serially correlated.

H1: Data are serially correlated.

```{r, comment=NA, warning=FALSE, message=FALSE,fig.height=3}
checkresiduals(fit1)
```

---
# Step 6: Residual diagnostics (cont.)

```{r, comment=NA, warning=FALSE, message=FALSE}
fit1 %>% residuals() %>% ggtsdisplay()
```
---
# Step 6: Residual diagnostics (cont.)

```{r, comment=NA, warning=FALSE, message=FALSE, fig.height=4}
checkresiduals(fit3)
```

---
# Step 6: Residual diagnostics (cont.)

```{r, comment=NA, warning=FALSE, message=FALSE}
fit3 %>% residuals() %>% ggtsdisplay()
```
---

# Step 7: Calculate forecasts

.pull-left[
$ARIMA(0,1,1)(0,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}
fit1 %>% forecast(h=36) %>% 
  autoplot()

```
]

.pull-right[
$ARIMA(0,1,1)(1,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}
fit3 %>% forecast(h=36) %>% 
  autoplot()

```

]
---


$ARIMA(0,1,1)(0,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}
fit1.forecast <- fit1 %>% forecast(h=36) 
fit1.forecast$mean

```


$ARIMA(0,1,1)(1,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}
fit3.forecast <- fit3 %>% forecast(h=36) 
fit3.forecast$mean
```


---

# Step 8: Evaluate forecast accuracy

## How well our model is doing for out-of-sample?


<!--So far we have talked about fitted values and residuals.-->

<!--Train data and Test data. We want to know if forecasts doing well for out-of-sample.-->

Forecast error = True value - Observed value

$$e_{T+h}=Y_{T+h}-\hat{Y}_{T+h|T}$$

Where,

$Y_{T+h}$: $(T+h)^{th}$ observation, $h=1,..., H$

$\hat{Y}_{T+h|T}$: Forecast based on data uo to time $T$.

- **True** forecast error as the test data is not used in computing $\hat{Y}_{T+h|T}$.

- Unlike, residuals, forecast errors on the test set involve multi-step forecasts.

- Use forecast error measures to evaluate the models.

<!--Since, true forecast error, no hat involved.-->


---

# Step 7: Evaluate forecast accuracy


$ARIMA(0,1,1)(0,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}
fit1.forecast <- fit1 %>% 
  forecast(h=36) 

```

```{r, comment=NA}
accuracy(fit1.forecast$mean, test.ap)
```



$ARIMA(0,1,1)(1,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}
fit3.forecast <- fit3 %>% 
  forecast(h=36) 

```

```{r, comment=NA}
accuracy(fit3.forecast$mean, test.ap)
```


$ARIMA(0,1,1)(0,1,1)_{12}$ MAE, MAPE is smaller than $ARIMA(0,1,1)(1,1,1)_{12}$. Hence, we select $ARIMA(0,1,1)(0,1,1)_{12}$ to forecast future values.

---
class: duke-orange, center, middle

# Automated ARIMA algorithm: `auto.arima`

---

# Modelling steps: `auto.arima` 

1. Plot the data.

1. If necessary, transform the data (using a Box-Cox transformation) to stabilise the variance.

1. ~~If the data are non-stationary, take first differences of the data until the data are stationary.~~

1. ~~Examine the ACF/PACF to identify a suitable model.~~

1. ~~Try your chosen model(s), and use the AICc to search for a better model.~~

1. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.

1. Once the residuals look like white noise, calculate forecasts.

---

#  Modelling steps: `auto.arima`

1. Plot the data.

1. If necessary, transform the data (using a Box-Cox transformation) to stabilise the variance.

1. **Use `auto.arima` to select a model.**

1. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.

1. Once the residuals look like white noise, calculate forecasts.
--
# Introduction: `auto.arima`

- Hyndman and Khandakar ([JSS, 2008](/slides/v27iO3.pdf)) algorithm.

- Select no differences **d** and **D** via KPSS test and strength of seasonality measurement.

- Select **p, q** by minimising AICc.

- Use stepwise search to traverse model space.

---

**What is happening under the hood of `auto.arima`?**
--

**Step 1:** Select the number of differences d and D via unit root tests and strength of seasonality measure.
--

**Step 2:** Try four possible models to start with:

i) $ARIMA(2, d, 2)$ if $m = 1$ and $ARIMA(2, d, 2)(1, D, 1)_m$ if $m > 1$.

ii) $ARIMA(0, d, 0)$ if $m = 1$ and $ARIMA(0, d, 0)(0, D, 0)_m$ if $m > 1$.

iii) $ARIMA(1, d, 0)$ if $m = 1$ and $ARIMA(1, d, 0)(1, D, 0)_m$ if $m > 1$.

iv) $ARIMA(0, d, 1)$ if $m = 1$ and $ARIMA(0, d, 1)(0, D, 1)_m$ if $m > 1$.
--

**Step 3:** Select the model with the smallest AICc from step 2. This becomes the current model.
--

**Step 4:** Consider up to 13 variations on the current model:

i) Vary one of $p, q, P$ and $Q$ from the current model by $\pm 1$.

ii) $p, q$ both vary from the current model by $\pm 1$.

iii) $P, Q$ both vary from the current model by $\pm 1$.

iv) Include or exclude the constant term from the current model. Repeat step 4 until no lower AICc can be found. 


---

# `auto.arima` with AirPassenger 

```{r, comment=NA}
fit.auto.arima <- auto.arima(training.ap, lambda = 0)
fit.auto.arima
```

Your turn:

 > Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.
 
 > Calculate forecasts.

---

# Check the residuals

```{r, comment=NA, message=FALSE, fig.height=5}
fit.auto.arima %>% checkresiduals()
```

---

```{r, comment=NA, message=FALSE, fig.height=7}
fit.auto.arima %>% residuals() %>% ggtsdisplay()
```
---

### Change the number of lags and perform `Box.test` (optional)

Note that in `fit.auto.arima %>% checkresiduals()` performed the test for 22 lags.

```{r, comment=NA, message=FALSE}
# lag=2m where m is the period of seasonality
# https://robjhyndman.com/hyndsight/ljung-box-test/
fit.auto.arima.resid <- fit.auto.arima %>% residuals()
Box.test(fit.auto.arima.resid, lag = 24, type = "Ljung-Box")
Box.test(fit.auto.arima.resid, lag = 24, type = "Box-Pierce")
```
---

# Forecasting

```{r, comment=NA, message=FALSE, fig.height=7}
fit.auto.arima %>% forecast(h=36) %>% autoplot()
```

---

## Structure of the forecasting output

```{r, comment=NA, message=FALSE, fig.height=7}
forecast.auto.arima <- fit.auto.arima %>% forecast(h=36) 
str(forecast.auto.arima)

```
---

# Extract forecasts

```{r, comment=NA}
forecast.auto.arima$mean
head(forecast.auto.arima$lower, 3)
head(forecast.auto.arima$upper, 3)
```

---

# Plot true values and forecasts

```{r, fig.height=5}
forecast.auto.arima %>%
  autoplot() +
  geom_line(
    aes(
      x = as.numeric(time(test.ap)),
      y = as.numeric(test.ap)
    ),
    col = "red")

```

---

```{r, comment=NA, fig.height=4}
ggplot() +
  geom_line(
    aes(
      x = as.numeric(time(forecast.auto.arima$mean)),
      y = as.numeric(forecast.auto.arima$mean)
    ), col = "blue") +
  geom_line(
    aes(
      x = as.numeric(time(test.ap)),
      y = as.numeric(test.ap)
    ), col = "red")
```

---

## Forecast accuracy

```{r, comment=NA, message=FALSE}
accuracy(forecast.auto.arima, test.ap)
```
---

## Multi-step ahead forecast

```r
fit.auto.arima %>% forecast(h=36)
```

This repeatedly feeds the **predicted data point** back into the prediction equation to get the next prediction.

<!--Predict the next value. Fit that back to data and refit the original model(same parameter estimates) and then forecast the next. Next forecast is again feed back and refit the original model and predict the next.-->

<!--Resources: https://stats.stackexchange.com/questions/217955/difference-between-first-one-step-ahead-forecast-and-first-forecast-from-fitted-->

---
background-image: url('multi.PNG')
background-position: center
background-size: contain

---
background-image: url('one.PNG')
background-position: center
background-size: contain
---
## One-step ahead forecast.

Continually updating the prediction equation with **new data**.


```{r, comment=NA}
air_model_test <- Arima(test.ap, model = fit.auto.arima)
# Uses the same coef as the air_model. Nothing is actually "refit"
coef(fit.auto.arima) # Model from auto.arima
coef(air_model_test) # Refit the same model to test

```

---

# Obtain one-step ahead forecast

```{r, comment=NA, message=FALSE, warning=FALSE}
one_step_forecasts <- fitted(air_model_test)
one_step_forecasts
```

---

# Comparison between one-step ahead forecasts and true values

```{r, comment=NA, fig.height=4}
ggplot() +
  geom_line(
    aes(
      x = as.numeric(time(one_step_forecasts)),
      y = as.numeric(one_step_forecasts)
    ), col = "black") +
  geom_line(
    aes(
      x = as.numeric(time(test.ap)),
      y = as.numeric(test.ap)
    ), col = "red")
```

---

## Comparison of accuracy: Multi-step ahead vs one-step ahead

```{r, comment=NA}
accuracy(one_step_forecasts, test.ap) # One-step ahead
accuracy(forecast.auto.arima, test.ap) # Multi-step ahead
```


---

## Multi-step ahead vs One-step ahead

.pull-left[

# Multi-step


```{r, comment=NA, echo=FALSE}
ggplot() +
  geom_line(
    aes(
      x = as.numeric(time(forecast.auto.arima$mean)),
      y = as.numeric(forecast.auto.arima$mean)
    ), col = "blue") +
  geom_line(
    aes(
      x = as.numeric(time(test.ap)),
      y = as.numeric(test.ap)
    ), col = "red")
```


]

.pull-right[

# One-step ahead

```{r, comment=NA, echo=FALSE}
ggplot() +
  geom_line(
    aes(
      x = as.numeric(time(one_step_forecasts)),
      y = as.numeric(one_step_forecasts)
    ), col = "black") +
  geom_line(
    aes(
      x = as.numeric(time(test.ap)),
      y = as.numeric(test.ap)
    ), col = "red")
```


]

blue - Multi-step ahead; black - One-step ahead; red - True values

---
background-image: url('arimaflowchart.png')
background-position: right
background-size: contain

.footnote[.scriptsize[
source: Forecasting: Principles and Practice, Rob Hyndman and George Athanasopoulos.
]]

---

# Comparison of accuracy measures of different approaches

## ARIMA - manual selection

```{r, comment=NA}
accuracy(fit1.forecast$mean, test.ap) # Multiple-step ahead
```

You can try one-step ahead forecast for this one too.

## `auto.arima`

```{r, comment=NA}
accuracy(one_step_forecasts, test.ap) # One-step ahead
accuracy(forecast.auto.arima, test.ap) # Multi-step ahead
```



---
class: center, middle



All rights reserved by [Thiyanga S. Talagala](https://thiyanga.netlify.app/)